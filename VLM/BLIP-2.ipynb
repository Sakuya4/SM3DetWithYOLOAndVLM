{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2d548",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y salesforce-lavis\n",
    "!pip install -U \"transformers>=4.33\" accelerate sentencepiece pillow\n",
    "!pip install -q transformers accelerate torchvision\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26fed9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"loading pre-train processor and model\"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"  \n",
    "processor = Blip2Processor.from_pretrained(MODEL_ID, force_download=True)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\"\"\"loading pre-train processor and model done\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1aed2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_illegal_parking(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    prompt = (\n",
    "        \"Describe whether the image shows illegal parking. Answer yes or no, and explain the reason in one sentence.\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(images=image, text=[prompt], return_tensors=\"pt\").to(device)\n",
    "    eos_token_id = processor.tokenizer.eos_token_id\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=eos_token_id,\n",
    "        repetition_penalty=1.3\n",
    "    )\n",
    "    #print(\"generated_ids:\", generated_ids)\n",
    "    output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    if output.startswith(prompt):\n",
    "        output = output[len(prompt):].strip()\n",
    "\n",
    "    print(output)\n",
    "\n",
    "analyze_illegal_parking(\"./image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fa30c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image = Image.open(\"./image.png\").convert(\"RGB\")\n",
    "def vqa_illegal_check(model, processor, rawImg, device=\"cpu\"):\n",
    "    # ---- Step 1: 僅判斷 Yes/No ----\n",
    "    q1 = \"In the first image, is any visible car parked illegally? Answer only Yes or No.\"\n",
    "    inp1 = processor(images=rawImg, text=q1, return_tensors=\"pt\")\n",
    "    inp1 = {k: v.to(device) for k, v in inp1.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out1 = model.generate(\n",
    "            **inp1,\n",
    "            max_new_tokens=3,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    in_len1 = inp1[\"input_ids\"].shape[1]\n",
    "    ans1 = processor.decode(out1[0, in_len1:], skip_special_tokens=True).strip()\n",
    "\n",
    "    first = ans1.split()[0].strip(\",. \").capitalize() if ans1 else \"\"\n",
    "    if first not in {\"Yes\", \"No\"}:\n",
    "        full1 = processor.decode(out1[0], skip_special_tokens=True)\n",
    "        first = \"Yes\" if \"yes\" in full1.lower() else (\"No\" if \"no\" in full1.lower() else \"No\")\n",
    "\n",
    "    illegal = first\n",
    "\n",
    "    # ---- Step 2: 只要理由 ----\n",
    "    q2 = (\"Give one short reason based only on visible evidence \"\n",
    "          \"(e.g., red lines, blocking crosswalk/driveway, no-parking signs). \"\n",
    "          \"Answer format:\\nReason: \")\n",
    "    inp2 = processor(images=rawImg, text=q2, return_tensors=\"pt\")\n",
    "    inp2 = {k: v.to(device) for k, v in inp2.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out2 = model.generate(\n",
    "            **inp2,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    dec2_full = processor.decode(out2[0], skip_special_tokens=True)\n",
    "    reason = dec2_full.split(\"Reason:\", 1)[-1].strip()\n",
    "\n",
    "    junk = [\n",
    "        \"Briefly state the reason for your previous answer\",\n",
    "        \"based only on visible evidence\",\n",
    "        \"(e.g., red lines, blocking crosswalk, blocking driveway, no-parking signs)\",\n",
    "        \"Answer format\"\n",
    "    ]\n",
    "    low = reason.lower()\n",
    "    for j in junk:\n",
    "        low = low.replace(j.lower(), \"\")\n",
    "    reason = low.strip(\" :.-\\\"\").strip()\n",
    "\n",
    "    if not reason:\n",
    "        reason = \"????????????????????????????.\"\n",
    "\n",
    "    return f\"Illegal: {illegal}\\nReason: {reason}\"\n",
    "\n",
    "# ===== 使用 =====\n",
    "print(vqa_illegal_check(model, processor, image, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5459e8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"model visual question answering\"\"\"\n",
    "inputs_captioning = processor(rawImg, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids_captioning = model.generate(**inputs_captioning)\n",
    "generated_text_captioning = processor.batch_decode(generated_ids_captioning, skip_special_tokens=True)[0].strip()\n",
    "print(\"Image Captioning:\", generated_text_captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8034b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Q1\n",
    "question = \"Question: How many cars are in the first sub-image? Answer:\"\n",
    "inputs_vqa = processor(rawImg, text=question, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids_vqa = model.generate(**inputs_vqa, max_new_tokens=15, num_beams=3, do_sample=False, temperature=0.1)\n",
    "generated_text_vqa = processor.batch_decode(generated_ids_vqa, skip_special_tokens=True)[0].strip()\n",
    "print(\"VQA (first sub-image):\", generated_text_vqa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a094b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Q2\n",
    "question2 = \"Question: How many cars are in the second sub-image? Answer:\"\n",
    "inputs_vqa2 = processor(rawImg, text=question2, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids_vqa2 = model.generate(**inputs_vqa2, max_new_tokens=15, num_beams=3, do_sample=False, temperature=0.1)\n",
    "generated_text_vqa2 = processor.batch_decode(generated_ids_vqa2, skip_special_tokens=True)[0].strip()\n",
    "print(\"VQA (second sub-image):\", generated_text_vqa2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
